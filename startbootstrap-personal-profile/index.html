<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Saumya Jetley</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet">
    <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/resume.min.css" rel="stylesheet">
    <style>
    a {font-family: "Times New Roman", cursive}
    body,h1,h2,h3,h4,h5,h6,div,p, .subheading {font-family: "Times New Roman", cursive}
    h6 {color: #727b84;text-transform: capitalize;font-size: 1.3rem;}
    ul {
    list-style-type: '- ';
    }
    </style>

  </head>

  <body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Saumya Jetley</span>
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/gitim.jpeg" alt="">
        </span>
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#home">Home</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#news">News</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#projects">Projects</a>
          </li>
          <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#publications">Publications</a>
          </li>
          <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#datasets">Datasets</a>
          </li>
          <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#talks">Talks</a>
          </li>
          <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#teaching">Teaching</a>
          </li>
          <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#admin">Admin/Review Work</a>
          </li>
          <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#outreach">Outreach</a>
          </li>
          <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#awards">Awards</a>
          </li>
        </ul>
      </div>
    </nav>

    <div class="container-fluid p-0">

      <section class="resume-section p-3 p-lg-5 d-flex d-column" id="home">
        <div class="my-auto">
          <h1 class="mb-0">Saumya
            <span class="text-primary">Jetley</span>
          </h1>
          <div class="lead mb-5">Department of Engineering Science, University of Oxford - OX1 3PJ
            &nbsp;&nbsp; <a href="mailto:name@email.com">sjetley@robots.ox.ac.uk</a>
          </div>
          <p class="lead mb-5">I am a final year PhD student under <a href=http://www.robots.ox.ac.uk/~phst/ style="text-decoration:none">Prof. Philip Torr</a> generously funded by the <a href=https://cordis.europa.eu/project/rcn/110460_en.html style="text-decoration:none">ERC research grant</a> to pursue research in 'AI for scene understanding' at the University of Oxford.</p>

          <p class="lead mb-5">My research covers the broad themes of computer vision, machine learning and deep learning. I have worked on topics of human saliency estimation, object recognition, instance segmentation and attention modelling in deep neural nets. My recent work focuses on explaining the adversarial phenomenon observed in deep neural networks for image classification. 
</p>

          <div class="social-icons">
            <a href="https://www.linkedin.com/in/saumya-jetley-974aa69b/">
              <i class="fab fa-linkedin-in"></i>
            </a>
            <a href="https://github.com/saumya-jetley">
              <i class="fab fa-github"></i>
            </a>
            <a href="https://twitter.com/saumyajetley">
              <i class="fab fa-twitter"></i>
            </a>
<!--        <a href="#">
              <i class="fab fa-facebook-f"></i>
            </a>
-->
          </div>
        </div>
      </section>

      <hr class="m-0">

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="news">
        <div class="my-auto">
          <h2 class="mb-5">News</h2>
          <p class="lead mb-5">
<ul class="lead mb-5">
<li>    10/18: Excited to be giving a talk on my research work and academic journey at an <a href="https://www.eventbrite.com/e/women-in-ai-advancing-your-research-tickets-50703045173?utm_campaign=reminder_attendees_48hour_email&utm_medium=email&utm_source=eb_email&utm_term=eventname#">event</a> jointly organised by <a href="https://www.joinef.com/">EF</a> and <a href="https://www.researcherscode.com/">Researc/hers code</a>. Slides <a href="https://docs.google.com/presentation/d/13DurD8qbIuk28UCtX0VywebAmSjKnjOS82axhFQ9_4E/edit?usp=sharing">here</a>.<il>

<li>    10/18: Grateful to be receiving the WiML travel award, looking forward to the workshop.<il>

<li>    10/18: Grateful to be receiving the NIPS travel award.<il>

<li>    09/18: Our paper explaining adversarial vulnerability in deep neural nets is accepted at NIPS'18. 
    (<a href="https://nips.cc/Conferences/2018/Schedule?showEvent=12016">With friends like these..</a>)<il>

<li>    08/18: I am co-organising the <a href="http://www.deeplearningindaba.com/computer-vision-parallel-track.html">'Frontiers of Computer Vision'</a> track at the <a href="http://www.deeplearningindaba.com/2018.html"> Deep Learning Indaba'18</a>.<il>

<li>    06/18: I will be the <a href="https://www.cs.ox.ac.uk/societies/women/committee.html">outreach officer</a> with <a href="https://www.cs.ox.ac.uk/societies/women/">OxWoCS</a> for the academic year 2018-19. Our relationship history <a href="./news/oxwocs.html">here</a>!<il>

<li>    05/18: I am co-organising a <a href = "https://www.hayfestival.com/p-13591-bushra-alahmadi-klaudia-krawiecka-saumya-jetley.aspx">coding workshop</a> at the <a href="http://www.hayfestival.com/wales/home">Hay Science Festival</a>.<il>

<li>    05/18: I am presenting my work at <a href=https://iclr.cc/Conferences/2018/Schedule?type=Poster>ICLR'18</a>. Some highlights <a href=./news/iclr.html>here</a>.<il>

<li>    04/18: Grateful to be receiving <a href=https://www.stx.ox.ac.uk/about-st-cross>St. Cross</a> travel and research scholarship.<il>

<li>    03/18: Grateful to be receiving the ICLR'18 travel award.<il>

<li>    01/18: Our paper on learning attention in classification CNNs is accepted at ICLR'18. 
    (<a href="https://openreview.net/forum?id=HyzbhfWRW"> Learn to pay attention!</a>) <il>
</ul>
    <br/><br/>
    <a href='./news/about.html'>Older news</a>
    </p>
    </div>
    </section>


      <hr class="m-0">

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="projects">
        <div class="my-auto">
          <h2 class="mb-5">Projects</h2>

<ul>
    <li><h6 class="mb-5">Explaining Adversariality in Deep Nets</h6><il>
    <img src="./projects/traces-1.png" width="50%">
    <p class="lead"> <br/> The vulnerability of deep image classification networks to adversarial attack is now well known, but less well understood. Via a novel experimental analysis, we illustrate some facts about deep convolutional networks (DCNs) that shed new light on their behaviour and how it connects to the problem of adversaries.
In short, the celebrated performance of DCNs and their vulnerability to adversarial attack are simply two sides of the same coin: the input image-space directions along which the networks are most vulnerable to attack are the same directions which they leverage to achieve their classification performance in the first place.
We develop this result in two main steps. The first involves connecting universal adversarial perturbations to specific target classes by examining the properties of class-score outputs of nets as functions of 1D movements along specific directions in the input image space.
The second is a clear demonstration of the tight coupling between classification performance and vulnerability to adversarial attack within the spaces spanned by these directions.
<br/> [<a href="https://arxiv.org/abs/1807.04200">details</a>]
      [<a >code:forthcoming</a>]
      [<a >demo:forthcoming</a>]
    </p><h6 class="mb-5"></h6>

    <li><h6 class="mb-5">Learning to Pay Attention</h6><il>
    <img src="./projects/multi_level_attention-1.png" width="50%">
    <p class="lead"> <br/> We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures for image classification.  The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parameterised by the score matrices, must alone be used for classification. Our experimental observations provide clear evidence to the effect that the learned scores simulate 'attention'by amplifying the relevant and suppressing the irrelevant or misleading regions of the input image. Thus, the proposed function is able to bootstrap standard CNN architectures for the task of image classification and demonstrate superior generalisation.
<br/> [<a href="https://openreview.net/forum?id=HyzbhfWRW">details</a>]
      [<a href="https://github.com/saumya-jetley/cd_ICLR18_ActiveAttention">code</a>]
      [<a >demo:forthcoming</a>]
    </p><h6 class="mb-5"></h6>

    <li><h6 class="mb-5">Realtime Instance Segmentation</h6><il>
    <img src="./projects/straight_to-shapes.png" width="50%">
    <p class="lead"> <br/> In this work, we propose to directly regress to objects' shapes in addition to the object bounding boxes and categories. It is crucial to find an appropriate shape representation that is compact and decodable, and in which objects can be compared for higher-order concepts such as view similarity, pose variation and occlusion. To achieve this, we use a denoising convolutional auto-encoder to establish an embedding space, and place the decoder after a fast end-to-end network trained to regress directly to the encoded shape vectors. This yields what to the best of our knowledge is the first real-time shape prediction network, running at ~35 FPS on a high-end desktop, that additionally generalises to unseen categories. 
<br/> [<a href="https://arxiv.org/abs/1611.07932">details</a>]
      [<a href="https://github.com/torrvision/straighttoshapes">code</a>]
      [<a href="https://www.youtube.com/embed/zWZowrpaZ1c?ecver=1">video demo</a>]
    </p><h6 class="mb-5"></h6>

    <li><h6 class="mb-5">Human Saliency Estimation</h6><il>
    <img src="./projects/saliency-estimation.png" width="50%">
    <p class="lead"> <br/>  In this work, we introduce a new saliency map model which formulates a map as a generalized Bernoulli distribution. We then train a deep architecture to predict such maps using novel loss functions which pair the softmax activation function with measures designed to compute distances between probability distributions. We show in extensive experiments the effectiveness of such loss functions over standard ones on four public benchmark datasets, and demonstrate improved performance over state-of-the-art saliency methods. 
<br/> [<a href="https://ieeexplore.ieee.org/document/7780989">details</a>]
      [<a href="https://github.com/saumya-jetley/cd_Saliency_PDP">code</a>]
      [<a href="https://www.youtube.com/watch?v=H9CSwiY_ApU">video</a>]
    </p><h6 class="mb-5"></h6>

    <li><h6 class="mb-5">Leveraging Prototypical Priors</h6><il>
    <img src="./projects/prototypical_priors-1.png" width="50%">
    <p class="lead"> <br/> Recent works on zero-shot learning make use of side information such as visual attributes or natural language semantics to define the relations between output visual classes and then use these relationships to draw inference on new unseen classes at test time. In a novel extension to this idea, we propose the use of visual prototypical concepts as side information. For most real-world visual object categories, it may be difficult to establish a unique prototype. However, in cases such as traffic signs, brand logos, flags, and even natural language characters, these prototypical templates are available and can be leveraged for an improved recognition performance. Using prototypes as prior information, the deepnet pipeline learns the input image projections into the prototypical embedding space subject to minimization of the final classification loss. Based on our experiments, prototypical embeddings incorporated in a conventional convolutional neural network improve the recognition performance. The same system can be directly deployed to draw inference on unseen classes by simply adding the prototypical information for these new classes at test time.
<br/> [<a href="https://arxiv.org/abs/1512.01192">details</a>]
      [<a href="https://github.com/saumya-jetley/cd_Prototypical_Priors_BMVC15">code</a>]
      [<a >demo:forthcoming</a>]<br/><br/></p><h6 class="mb-5"></h6>

</ul>
</div>
</section>



      <hr class="m-0">

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="publications">
        <div class="my-auto">
          <h2 class="mb-5">Publications</h2>

    <div class="lead mb-5">

      <h6> [2018] </h6>

      <b> With Friends Like These, Who Needs Adversaries?,</b> 
      Saumya Jetley*, Nicholas A. Lord*, Philip H.S. Torr,
      <i>Proceedings of the 32nd conference on Neural Information Processing Systems (NIPS) 2018</i>
      [<a href="https://arxiv.org/abs/1807.04200">pdf</a>]
      [<a >code:forthcoming</a>]
	  [<a href="./bibs/withfriendslikethese.html">bibtex</a>]<br/><br/>

      <b> Learn to pay attention, </b> 
      Saumya Jetley, Nicholas A. Lord, Namhoon Lee, Philip H.S. Torr,
      <i>Proceedings of the 6th International conference on learning representations (ICLR) 2018</i>
      [<a href="https://arxiv.org/pdf/1804.02391.pdf">pdf</a>]
      [<a href="https://github.com/saumya-jetley/cd_ICLR18_ActiveAttention">code</a>][<a href="./bibs/learntopayattention.html">bibtex</a>]<br/><br/>

      
      <h6> [2017] </h6>
      
      <b> End-to-end saliency mapping via probability distribution prediction, </b> 
      Saumya Jetley, Naila Murray, Eleonora Vig,
      <i>U.S. Patent 9,830,529 with Xerox Corp (now Naver Labs)</i>
      [<a href="https://patents.google.com/patent/US9830529B2/en">details</a>]<br/><br/>

      <b> Straight to Shapes: Real-time Detection of Encoded Shapes, </b> 
      Saumya Jetley*, Michael Sapienza*, Stuart Golodetz, Philip H.S. Torr,
      <i>Proceedings of the International conference on Computer Vision and Pattern Recognition (CVPR) 2017</i>
      [<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Jetley_Straight_to_Shapes_CVPR_2017_paper.pdf">pdf</a>]
      [<a href="https://github.com/torrvision/straighttoshapes">code</a>][<a href="./bibs/straighttoshapes.html">bibtex</a>]<br/><br/>

      <h6> [2016] </h6>

      <b> End-to-End Saliency Mapping via Probability Distribution Prediction, </b> 
      Saumya Jetley, Naila Murray, Eleonora Vig,
      <i>Proceedings of the International conference on Computer Vision and Pattern Recognition (CVPR) 2016</i>
      [<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Jetley_End-To-End_Saliency_Mapping_CVPR_2016_paper.pdf">pdf</a>]
      [<a href="https://github.com/saumya-jetley/cd_Saliency_PDP">code</a>][<a href="./bibs/endtoendsaliencyestimation.html">bibtex</a>]<br/><br/>

      <h6> [2015] </h6>

      <b> Prototypical Priors: From Improving Classification to Zero-Shot Learning , </b> 
      Saumya Jetley, Bernardino Romera-Paredes, Sadeep Jayasumana, Philip H.S. Torr,
      <i>Proceedings of the British Machine Vision Conference (BMVC) 2015</i>
      [<a href="https://arxiv.org/abs/1512.01192">pdf</a>]
      [<a href="https://github.com/saumya-jetley/cd_Prototypical_Priors_BMVC15">code</a>][<a href="./bibs/prototypicalpriors.html">bibtex</a>]<br/><br/>

      <h6> [2014] </h6>

      <b> 3D Activity Recognition Using Motion History and Binary Shape Templates, </b> 
      Saumya Jetley, Fabio Cuzzolin,
      <i>Workshop proceedings of the Asian Conference on Computer Vision (ACCV) 2014 </i>
      [<a href="https://link.springer.com/chapter/10.1007/978-3-319-16628-5_10">pdf</a>][<a href="./bibs/3dactivityrecognition.html">bibtex</a>]<br/><br/>

      <b> Multi-script Identification from Printed Words, </b> 
      Saumya Jetley, Kapil Mehrotra, Atish Vaze, Swapnil Belhe,
      <i>proceedings of the International conference on Image Analysis and Recognition (ICIAR) 2014 </i>
      [<a href="https://link.springer.com/chapter/10.1007/978-3-319-11758-4_39">pdf</a>][<a href="./bibs/multiscriptidentification.html">bibtex</a>]<br/><br/>

      <h6> [2013] </h6>

      <b> Automatic flag recognition using texture based color analysis and gradient features , </b> 
      Saumya Jetley, Atish Vaze, Swapnil Belhe,
      <i>Proceedings of the International conference on Image Information Processing (ICIIP) 2013 </i>
      [<a href="https://ieeexplore.ieee.org/abstract/document/6707635/">pdf</a>][<a href="./bibs/automaticflagrecognition.html">bibtex</a>]<br/><br/>

      <b> Unconstrained handwritten Devanagari character recognition using convolutional neural networks, </b> 
      Kapil Mehrotra, Saumya Jetley, Akash Deshmukh, Swapnil Belhe,
      <i> Proceedings of the 4th International workshop on Multilingual OCR (MOCR) 2013</i>
      [<a href="https://dl.acm.org/citation.cfm?id=2505386">pdf</a>][<a href="./bibs/unconstraineddeccharreco.html">bibtex</a>]<br/><br/>

      <h6> [2012] </h6>

      <b> Hindi handwritten word recognition using HMM and symbol tree, </b> 
      Swapnil Belhe, Chetan Paulzagade, Akash Deshmukh, Saumya Jetley, Kapil Mehrotra,
      <i> Proceeding of the workshop on Document Analysis and Recognition (DAR) 2012 </i>
      [<a href="https://dl.acm.org/citation.cfm?id=2432556">pdf</a>][<a href="./bibs/hindihwwordreco.html">bibtex</a>]<br/><br/>

      <b> Two-Stage hybrid binarization around fringe map based text line segmentation for document images,</b> 
      Saumya Jetley, Swapnil Belhe, V.K. Koppula, Atul Negi, 
      <i> Proceedings of the 21st International Conference on Pattern Recognition (ICPR) 2012 </i>
      [<a href="https://ieeexplore.ieee.org/document/6460142/">pdf</a>]
	  [<a href="./bibs/twostagebinarisation.html">bibtex</a>]

    </div>
    </section>



      <hr class="m-0">

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="datasets">
        <div class="my-auto">
          <h2 class="mb-5">Datasets</h2>
          <p class="lead mb-5">
      Long long time ago, I gathered a dataset of 'Country Flags in the Wild'. It comprises of
      12,854 train images and 6,110 test images of the flags of 224 different countries harvested from the world wide web and manually cropped to loosely fit to the inlying flags. More details can be found in this <a href="https://ieeexplore.ieee.org/abstract/document/6707635">paper</a>. 
      The dataset itself can be downloaded from <a href="/data/tvg/sjetley/">here</a>.
          </p>
        </div>
      </section>



      <hr class="m-0">
      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="talks">
        <div class="my-auto">
        <h2 class="mb-5">Talks</h2>
        <p class="lead mb-5">
<ul class="lead mb-5">
<li>        10/18: I gave a talk on my research work and academic journey at an <a href="https://www.eventbrite.com/e/women-in-ai-advancing-your-research-tickets-50703045173?utm_campaign=reminder_attendees_48hour_email&utm_medium=email&utm_source=eb_email&utm_term=eventname#">event</a> jointly organised by <a href="https://www.joinef.com/">EF</a> and <a href="https://www.researcherscode.com/">Researc/hers code</a>. Find the slides <a href="https://docs.google.com/presentation/d/13DurD8qbIuk28UCtX0VywebAmSjKnjOS82axhFQ9_4E/edit?usp=sharing">here</a>.<il>

<li>        07/17: I gave a talk at the Cambridge office of <a href="https://five.ai/">FiveAI</a>. 
        Slides <a href="https://drive.google.com/file/d/1X_lZU-IkT2bdjyZ4QojfeGvodX_Tavde/view?usp=sharing">
        here</a>. Video link for the demo video on one of the slides is <a href="https://www.youtube.com/watch?v=zWZowrpaZ1c">here</a>.<il>

<li>        03/17: I was selected for a student talk at the <a href="http://workshops.inf.ed.ac.uk/deep/deep2017/">Deep Learning Workshop'17 in Edinburgh</a>. Access the <a href="https://www.youtube.com/watch?v=sTCY4JWPI4I">recording</a>, <a href="https://drive.google.com/file/d/1X_lZU-IkT2bdjyZ4QojfeGvodX_Tavde/view?usp=sharing"> slides</a>, and <a href="https://www.youtube.com/watch?v=zWZowrpaZ1c">demo</a>.<il>

    </p>
    </section>




      <hr class="m-0">

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="teaching">
        <div class="my-auto">
          <h2 class="mb-5">Teaching</h2>

        <h6 class="mb-4">Tutored for:</h6>
          <ul  class="lead mb-5">
            <li><a href="http://www.robots.ox.ac.uk/~fwood/teaching/B16_hilary_2013_2014/index.html">B16 course on Operating Systems (Hilary'16)</a><il>
            <li><a href="http://www.robots.ox.ac.uk/~az/lectures/ia/index.html">B14 course on Image Analysis (Michaelmas'16)</a><il>
            <li><a href="http://www.robots.ox.ac.uk/~sjrob/Teaching/B14_SP/b14_sp_lect1.pdf">B14 course on Signal Analysis (Hilary'17)</a><il>
            <li>B14 course on Image Analysis (Exam Preparatory course, Trinity'17)<il>
           </ul><br/>

        <h6 class="mb-4">Laboratory demonstrator for:</h6>
        <ul  class="lead mb-5">
        <li><a href="http://www.eng.ox.ac.uk/~labejp/Courses/1P5/">P5 Matlab software programming course (Michaelmas'16, Hilary'17, Trinity'17)</a><il>
        <li>B14 Lab course on Image Analysis (Hilary'17)<il>
        </ul>
      </section>




      <hr class="m-0">
      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="admin">
        <div class="my-auto">
        <h2 class="mb-5">Admin/Review Work</h2>
        <p class="lead mb-5">
<ul class="lead mb-5">
<li>        10/18: I am reviewing for ICLR'19.<il>

<li>        08/18: I co-organised the <a href="http://www.deeplearningindaba.com/computer-vision-parallel-track.html">'Frontiers of Computer Vision'</a> track at the <a href="http://www.deeplearningindaba.com/2018.html"> Deep Learning Indaba'18</a>.<il>

<li>        07/18: I reviewed for NIPS'18.<il>        
</ul>
    </p>
    </section>



      <hr class="m-0">
      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="outreach">
        <div class="my-auto">
        <h2 class="mb-5">Outreach</h2>
        <p class="lead mb-5">
<ul class="lead mb-5">

<li>        09/18: I volunteered for the University-wide Open Day at the <a href="http://www.cs.ox.ac.uk/">CS department</a> and presented a realtime demo of my work on realtime instance segmentation. A teaser <a href="./news/listentoSee_demo.html">here</a>.<il>

<li>        06/18 to present: I have taken the position of <a href="https://www.cs.ox.ac.uk/societies/women/committee.html">Outreach & Scholarship officer</a> with <a href="https://www.cs.ox.ac.uk/societies/women/">OxWoCS</a> for the academic year 2018-19.<il>

<li>        06/18: I co-organised a coding workshop using <a href="https://www.youtube.com/watch?v=0yQYr7CIxBc&feature=youtu.be">spheros</a> as part of the <a href="https://www.cs.ox.ac.uk/conferences/InspireHer/">InspireHer!</a> series.<il>
    
<li>        05/18: I co-organised a <a href = "https://www.hayfestival.com/p-13591-bushra-alahmadi-klaudia-krawiecka-saumya-jetley.aspx">coding workshop</a> at the <a href="http://www.hayfestival.com/wales/home">Hay Science Festival</a>.<il>

<li>        06/17 to 05/18: I served as the Industry coordinator and External relations officer with 
        <a href="https://www.cs.ox.ac.uk/societies/women/">OxWoCS</a> for the academic year 2017-18. <il>

<li>        05/17 to 06/17: As a student outreach ambassador for the <a href="http://www.eng.ox.ac.uk/">Department of Engineering Science, University of Oxford</a>, I have engaged with yr. 11, 12 & 13 school students at a host of events including - <a href="https://www.oxengscioutreach.com/events">Lubbock lectures for schools, International Women in Engineering Day and Engineering Open Days</a>. One set of slides especially popular amongst students during my classroom talks can be found <a href="./news/AI_for_visualperception_slides.html">here</a>.<il>

<li>        06/17: I gave a talk under the theme of 'Mobile Robotics' for the <a href="https://www.cs.ox.ac.uk/lookingforward/">Looking Forward</a> initiative of the Department of Computer Science, University of Oxford to encourage and motivate girl students towards STEM.<il>

<li>        02/17 - 05/17: I served as a student volunteer with the <a href="https://www.sciencemuseum.org.uk/what-was-on/robots">Robots exhibition at the Science Museum London</a>, supported by the Royal Academy of Engineering. More details <a href="./news/robots_SML.html">here</a>.<il>

</ul>

    </p>
    </section>


      <hr class="m-0">
      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="awards">
        <div class="my-auto">
        <h2 class="mb-5">Awards</h2>
        <p class="lead mb-5">
<ul class="lead mb-5">

<li> 2017: <a href="https://www.pmb.ox.ac.uk/news/pembroke-team-win-tri-innovate-2017-innovative-audio-navigation-concept">Winner of the Tri-innovate challenge</a> - Oxford university annual innovation challenge for identifying and supporting the most upcoming early-stage business ideas.<il>

<li> 2016: Amongst the <a href="https://warwick.ac.uk/fac/sci/dcs/research/tia/her2contest/outcome">top-10 algorithms for breast cancer prognosis challenge</a> (lookout for Team HERcules).<il>

<li> 2015: Best research internship presentation for ‘End-to-end saliency prediction using deep learning’ project presentation at Xerox Research Centre Europe, Grenoble (now <a href="http://www.europe.naverlabs.com/NAVER-LABS-Europe/People/Naila-Murray">Naver Labs</a>).<il>

<li> 2015: Received an Honorary Mention for Originality for the <a href="https://drive.google.com/file/d/0B-5KrfSltQrGSXBWYkpCbThjdjQ/view">Technical Essay entry</a> at the International Computer Vision Summer School'15.<il>

<li> 2014-15: Recipient of the <a href="http://www.stapleytrust.org/wp/about/">Sir Richard Stapley academic scholarship</a>. <il>

<li> 2012: Best Performer– 2012 & Best Team – 2012 awardee at CDAC-Pune for outstanding research work.<il>

<li> 2010: Runner Up in Most Innovative Projects category at National Technical Symposium – <a href="http://pictinc.org/">Impetus & Concepts</a>'10.<il>

<li> 2006: All India Rank 102 in 5th National Cyber Olympiad of the <a href="http://www.sofworld.org/nco">Science Olympiad Foundation</a>.<il>

</ul>
        </p>
        </section>



    </div>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/resume.min.js"></script>

  </body>

</html>
