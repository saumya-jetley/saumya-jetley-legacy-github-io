<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Saumya Jetley</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet">
    <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/resume.min.css" rel="stylesheet">
    <style>
    a {font-family: "Times New Roman", cursive}
    body,h1,h2,h3,h4,h5,h6,div,p, .subheading {font-family: "Times New Roman", cursive}
    h6 {color: #727b84;text-transform: capitalize;font-size: 1.3rem;}
    ul {
    list-style-type: '- ';
    }
    </style>

  </head>

  <body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Saumya Jetley</span>
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/gitim.jpeg" alt="">
        </span>
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#home">Home</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#news">News</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#projects">Projects</a>
          </li>
          <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#publications">Publications</a>
          </li>
          <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#datasets">Datasets</a>
          </li>
          <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#talks">Talks</a>
          </li>
          <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#teaching">Teaching</a>
          </li>
          <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#admin">Reviewer duties</a>
          </li>
          <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#outreach">Outreach</a>
          </li>
          <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#awards">Awards</a>
          </li>
        </ul>
      </div>
    </nav>

    <div class="container-fluid p-0">

      <section class="resume-section p-3 p-lg-5 d-flex d-column" id="home">
        <div class="my-auto">
          <h1 class="mb-0">Saumya
            <span class="text-primary">Jetley</span>
          </h1>
          <div class="lead mb-5">Department of Engineering Science, University of Oxford - OX1 3PJ
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="mailto:name@email.com">sjetley@robots.ox.ac.uk</a>
          </div>
          <p class="lead mb-5">
            I joined <a href=http://www.robots.ox.ac.uk/~phst/ style="text-decoration:none">Phil Torr's lab</a> at the university of Oxford as a masters (by research) student in Oct'14. By the end of a year, I had successfully transferred to a full-time PhD position fully supported by the <a href=https://cordis.europa.eu/project/rcn/110460_en.html style="text-decoration:none">ERC grant HELIOS</a>. I published 5 first-author publications during the course of my PhD and recently submitted my thesis titled - 'Use and Examination of CNNs for Scene Understanding.' My research focuses on applying deep convolutional networks to vision problems and studying the interpretability and robustness properties of these networks in this regime. 
<br/><br/>
In my spare time I run coding workshops for kids aged 9-12, write poetry, read semi-fictions, cook Indian curries and train in the classical Indian dance form of Bharatnatyam.
</p>

          <div class="social-icons">
            <a href="https://www.linkedin.com/in/saumya-jetley-974aa69b/">
              <i class="fab fa-linkedin-in"></i>
            </a>
            <a href="https://github.com/saumya-jetley">
              <i class="fab fa-github"></i>
            </a>
            <a href="https://twitter.com/saumyajetley">
              <i class="fab fa-twitter"></i>
            </a>
<!--        <a href="#">
              <i class="fab fa-facebook-f"></i>
            </a>
-->
          </div>
        </div>
      </section>

      <hr class="m-0">

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="news">
        <div class="my-auto">
          <h2 class="mb-5">News</h2>
          <p class="lead mb-5">
02/19: I will be a visiting researcher at <a href="https://five.ai/">FiveAI</a> starting this month.
<br/>
01/19: I submitted my thesis, wohoo! It is titled - 'Use and Examination of CNNs for Scene Understanding' (coming online soon).
<br/>
12/18: I gave a talk at the newly inaugurated <a href="https://www.wadhwaniai.org/">Wadhwani Institute, Mumbai</a>. 
Such energy, great leadership, AI for social good in Indian space, ftw!
</br>
10/18: Excited to be giving a talk on my research work and academic journey at an <a href="https://www.eventbrite.com/e/women-in-ai-advancing-your-research-tickets-50703045173?utm_campaign=reminder_attendees_48hour_email&utm_medium=email&utm_source=eb_email&utm_term=eventname#">event</a> jointly organised by <a href="https://www.joinef.com/">EF</a> and <a href="https://www.researcherscode.com/">Researc/hers code</a>. Slides <a href="https://docs.google.com/presentation/d/13DurD8qbIuk28UCtX0VywebAmSjKnjOS82axhFQ9_4E/edit?usp=sharing">here</a>.
<br/>
10/18: Grateful to be receiving the WiML travel award, looking forward to the workshop.
<br/>
10/18: Grateful to be receiving the NIPS travel award.
<br/>
09/18: Our paper explaining adversarial vulnerability in deep neural nets is accepted at NIPS'18. (<a href="https://nips.cc/Conferences/2018/Schedule?showEvent=12016">With friends like these..</a>)
<br/>
08/18: I am co-organising the <a href="http://www.deeplearningindaba.com/computer-vision-parallel-track.html">'Frontiers of Computer Vision'</a> track at the <a href="http://www.deeplearningindaba.com/2018.html"> Deep Learning Indaba'18</a>.
<br/>
06/18: I will be the <a href="https://www.cs.ox.ac.uk/societies/women/committee.html">outreach officer</a> with <a href="https://www.cs.ox.ac.uk/societies/women/">OxWoCS</a> for the academic year 2018-19. Our relationship history <a href="./news/oxwocs.html">here</a>!
<br/>
05/18: I am co-organising a <a href = "https://www.hayfestival.com/p-13591-bushra-alahmadi-klaudia-krawiecka-saumya-jetley.aspx">coding workshop</a> at the <a href="http://www.hayfestival.com/wales/home">Hay Science Festival</a>.
<br/>
05/18: I am presenting my work at <a href=https://iclr.cc/Conferences/2018/Schedule?type=Poster>ICLR'18</a>. Some highlights <a href=./news/iclr.html>here</a>.
<br/>
04/18: Grateful to be receiving <a href=https://www.stx.ox.ac.uk/about-st-cross>St. Cross</a> travel and research scholarship.
<br/>
03/18: Grateful to be receiving the ICLR'18 travel award.
<br/>
01/18: Our paper on learning attention in classification CNNs is accepted at ICLR'18. (<a href="https://openreview.net/forum?id=HyzbhfWRW"> Learn to pay attention!</a>)
<br/>
</p>

    <br/>
    <a href='./news/about.html'>Older news</a>
    </p>
    </div>
    </section>


      <hr class="m-0">

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="projects">
        <div class="my-auto">
          <h2 class="mb-5">Projects</h2>

<ul>
    <li><h5 class="mb-5">With Friends Like These, Who Needs Adversaries?</h5>
    <il>
    <img src="./projects/traces-1.png" width="50%">
    <p class="lead"> <br/> 
This work performs a principled analysis of the class decision functions learned by classification CNNs by contextualising an existing geometrical framework for network decision boundary analysis. 
Our research uncovers some very intriguing yet simplistic facets of the class score functions learned by these networks that explain their adversarial vulnerability.
We identify the fact that specific input image space directions tend to be associated with fixed class identities. 
This means that simply increasing the magnitude of correlation between the input image and a single image space direction causes the nets to believe that more (or less) of the class is present. 
This allows us to provide a new perspective on the existence of universal adversarial perturbations.
Further, the input image space directions which the networks use to achieve their classification performance are the same along which the networks are most vulnerable to attack; the vulnerability arises from the rather simplistic non-linear use of the directions. Thus, as it stands, the performance and vulnerability of these nets are closely entwined.
Various notable observations emerge from this, one of which is that any attempt to make a trained network more robust to an adversarial attack by suppressing input data dimensions or intermediate network features would always be accompanied by a corresponding loss in network accuracy. 
Moreover, if the attack is optimised to take into account the suppression of dimensions, it regains its effectiveness.
These results present key implications for future efforts to construct neural nets that are both accurate and robust to adversarial attack.
<br/> [<a href="https://arxiv.org/abs/1807.04200">paper</a>]
      [<a >code</a>]
      [<a href="https://www.youtube.com/watch?v=hOQdyInhYi0">video</a>]
    </p><h6 class="mb-5"></h6>

    <li><h5 class="mb-5">Learning to Pay Attention</h5><il>
    <img src="./projects/multi_level_attention-1.png" width="50%">
    <p class="lead"> <br/> We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures for image classification.  The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parameterised by the score matrices, must alone be used for classification. Our experimental observations provide clear evidence to the effect that the learned scores simulate 'attention'by amplifying the relevant and suppressing the irrelevant or misleading regions of the input image. Thus, the proposed function is able to bootstrap standard CNN architectures for the task of image classification and demonstrate superior generalisation.
<br/> [<a href="https://openreview.net/forum?id=HyzbhfWRW">paper</a>]
      [<a href="https://github.com/saumya-jetley/cd_ICLR18_ActiveAttention">code</a>]
      [<a >video</a>]
    </p><h6 class="mb-5"></h6>

    <li><h5 class="mb-5">Real-time Instance Segmentation</h5><il>
    <img src="./projects/straight_to-shapes.png" width="50%">
    <p class="lead"> <br/> In this work, we propose to directly regress to objects' shapes in addition to the object bounding boxes and categories. It is crucial to find an appropriate shape representation that is compact and decodable, and in which objects can be compared for higher-order concepts such as view similarity, pose variation and occlusion. To achieve this, we use a denoising convolutional auto-encoder to establish an embedding space, and place the decoder after a fast end-to-end network trained to regress directly to the encoded shape vectors. This yields what to the best of our knowledge is the first real-time shape prediction network, running at ~35 FPS on a high-end desktop, that additionally generalises to unseen categories. 
<br/> [<a href="https://arxiv.org/abs/1611.07932">paper</a>]
      [<a href="https://github.com/torrvision/straighttoshapes">code</a>]
      [<a href="https://www.youtube.com/embed/zWZowrpaZ1c?ecver=1">video</a>]
    </p><h6 class="mb-5"></h6>

    <li><h5 class="mb-5">Human Saliency Estimation</h5><il>
    <img src="./projects/saliency-estimation.png" width="50%">
    <p class="lead"> <br/>  In this work, we introduce a new saliency map model which formulates a map as a generalized Bernoulli distribution. We then train a deep architecture to predict such maps using novel loss functions which pair the softmax activation function with measures designed to compute distances between probability distributions. We show in extensive experiments the effectiveness of such loss functions over standard ones on four public benchmark datasets, and demonstrate improved performance over state-of-the-art saliency methods. 
<br/> [<a href="https://ieeexplore.ieee.org/document/7780989">paper</a>]
      [<a href="https://github.com/saumya-jetley/cd_Saliency_PDP">code</a>]
      [<a href="https://www.youtube.com/watch?v=H9CSwiY_ApU">video</a>]
    </p><h6 class="mb-5"></h6>

    <li><h5 class="mb-5">Leveraging Prototypical Priors</h5><il>
    <img src="./projects/prototypical_priors-1.png" width="50%">
    <p class="lead"> <br/> Recent works on zero-shot learning make use of side information such as visual attributes or natural language semantics to define the relations between output visual classes and then use these relationships to draw inference on new unseen classes at test time. In a novel extension to this idea, we propose the use of visual prototypical concepts as side information. For most real-world visual object categories, it may be difficult to establish a unique prototype. However, in cases such as traffic signs, brand logos, flags, and even natural language characters, these prototypical templates are available and can be leveraged for an improved recognition performance. Using prototypes as prior information, the deepnet pipeline learns the input image projections into the prototypical embedding space subject to minimization of the final classification loss. Based on our experiments, prototypical embeddings incorporated in a conventional convolutional neural network improve the recognition performance. The same system can be directly deployed to draw inference on unseen classes by simply adding the prototypical information for these new classes at test time.
<br/> [<a href="https://arxiv.org/abs/1512.01192">paper</a>]
      [<a href="https://github.com/saumya-jetley/cd_Prototypical_Priors_BMVC15">code</a>]
      [<a >video</a>]<br/><br/></p><h6 class="mb-5"></h6>

</ul>
</div>
</section>



      <hr class="m-0">

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="publications">
        <div class="my-auto">
          <h2 class="mb-5">Publications</h2>

    <p class="lead mb-5">

      <h6> [2018] </h6>

      <b> With Friends Like These, Who Needs Adversaries?,</b> 
      Saumya Jetley*, Nicholas A. Lord*, Philip H.S. Torr,
      <i>Proceedings of the 32nd conference on Neural Information Processing Systems (NIPS) 2018</i>
      [<a href="https://arxiv.org/abs/1807.04200">pdf</a>]
      [<a >code</a>]
	  [<a href="./projects/NIPS_withfriendslikethese_poster.pdf">poster</a>]
      [<a href="./bibs/withfriendslikethese.html">bibtex</a>]<br/><br/>

      <b> Learn to pay attention, </b> 
      Saumya Jetley, Nicholas A. Lord, Namhoon Lee, Philip H.S. Torr,
      <i>Proceedings of the 6th International conference on learning representations (ICLR) 2018</i>
      [<a href="https://arxiv.org/pdf/1804.02391.pdf">pdf</a>]
      [<a href="https://github.com/saumya-jetley/cd_ICLR18_ActiveAttention">code</a>]
      [<a href="./bibs/learntopayattention.html">bibtex</a>]<br/><br/>

      
      <h6> [2017] </h6>
      
      <b> End-to-end saliency mapping via probability distribution prediction, </b> 
      Saumya Jetley, Naila Murray, Eleonora Vig,
      <i>U.S. Patent 9,830,529 with Xerox Corp (now Naver Labs)</i>
      [<a href="https://patents.google.com/patent/US9830529B2/en">details</a>]<br/><br/>

      <b> Straight to Shapes: Real-time Detection of Encoded Shapes, </b> 
      Saumya Jetley*, Michael Sapienza*, Stuart Golodetz, Philip H.S. Torr,
      <i>Proceedings of the International conference on Computer Vision and Pattern Recognition (CVPR) 2017</i>
      [<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Jetley_Straight_to_Shapes_CVPR_2017_paper.pdf">pdf</a>]
      [<a href="https://github.com/torrvision/straighttoshapes">code</a>]
      [<a href="./bibs/straighttoshapes.html">bibtex</a>]<br/><br/>

      <h6> [2016] </h6>

      <b> End-to-End Saliency Mapping via Probability Distribution Prediction, </b> 
      Saumya Jetley, Naila Murray, Eleonora Vig,
      <i>Proceedings of the International conference on Computer Vision and Pattern Recognition (CVPR) 2016</i>
      [<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Jetley_End-To-End_Saliency_Mapping_CVPR_2016_paper.pdf">pdf</a>]
      [<a href="https://github.com/saumya-jetley/cd_Saliency_PDP">code</a>]
      [<a href="./bibs/endtoendsaliencyestimation.html">bibtex</a>]<br/><br/>

      <h6> [2015] </h6>

      <b> Prototypical Priors: From Improving Classification to Zero-Shot Learning , </b> 
      Saumya Jetley, Bernardino Romera-Paredes, Sadeep Jayasumana, Philip H.S. Torr,
      <i>Proceedings of the British Machine Vision Conference (BMVC) 2015</i>
      [<a href="https://arxiv.org/abs/1512.01192">pdf</a>]
      [<a href="https://github.com/saumya-jetley/cd_Prototypical_Priors_BMVC15">code</a>]
      [<a href="./bibs/prototypicalpriors.html">bibtex</a>]<br/><br/>

      <h6> [2014] </h6>

      <b> 3D Activity Recognition Using Motion History and Binary Shape Templates, </b> 
      Saumya Jetley, Fabio Cuzzolin,
      <i>Workshop proceedings of the Asian Conference on Computer Vision (ACCV) 2014 </i>
      [<a href="https://link.springer.com/chapter/10.1007/978-3-319-16628-5_10">pdf</a>]
      [<a href="./bibs/3dactivityrecognition.html">bibtex</a>]<br/><br/>

      <b> Multi-script Identification from Printed Words, </b> 
      Saumya Jetley, Kapil Mehrotra, Atish Vaze, Swapnil Belhe,
      <i>proceedings of the International conference on Image Analysis and Recognition (ICIAR) 2014 </i>
      [<a href="https://link.springer.com/chapter/10.1007/978-3-319-11758-4_39">pdf</a>]
      [<a href="./bibs/multiscriptidentification.html">bibtex</a>]<br/><br/>

      <h6> [2013] </h6>

      <b> Automatic flag recognition using texture based color analysis and gradient features , </b> 
      Saumya Jetley, Atish Vaze, Swapnil Belhe,
      <i>Proceedings of the International conference on Image Information Processing (ICIIP) 2013 </i>
      [<a href="https://ieeexplore.ieee.org/abstract/document/6707635/">pdf</a>]
      [<a href="./bibs/automaticflagrecognition.html">bibtex</a>]<br/><br/>

      <b> Unconstrained handwritten Devanagari character recognition using convolutional neural networks, </b> 
      Kapil Mehrotra, Saumya Jetley, Akash Deshmukh, Swapnil Belhe,
      <i> Proceedings of the 4th International workshop on Multilingual OCR (MOCR) 2013</i>
      [<a href="https://dl.acm.org/citation.cfm?id=2505386">pdf</a>]
      [<a href="./bibs/unconstraineddeccharreco.html">bibtex</a>]<br/><br/>

      <h6> [2012] </h6>

      <b> Hindi handwritten word recognition using HMM and symbol tree, </b> 
      Swapnil Belhe, Chetan Paulzagade, Akash Deshmukh, Saumya Jetley, Kapil Mehrotra,
      <i> Proceeding of the workshop on Document Analysis and Recognition (DAR) 2012 </i>
      [<a href="https://dl.acm.org/citation.cfm?id=2432556">pdf</a>]
      [<a href="./bibs/hindihwwordreco.html">bibtex</a>]<br/><br/>

      <b> Two-Stage hybrid binarization around fringe map based text line segmentation for document images,</b> 
      Saumya Jetley, Swapnil Belhe, V.K. Koppula, Atul Negi, 
      <i> Proceedings of the 21st International Conference on Pattern Recognition (ICPR) 2012 </i>
      [<a href="https://ieeexplore.ieee.org/document/6460142/">pdf</a>]
	  [<a href="./bibs/twostagebinarisation.html">bibtex</a>]

    </p>
    </section>



      <hr class="m-0">

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="datasets">
        <div class="my-auto">
          <h2 class="mb-5">Datasets</h2>
          <p class="lead mb-5">
      Long long time ago, I gathered a dataset of 'Country Flags in the Wild'. It comprises of
      12,854 train images and 6,110 test images of the flags of 224 different countries harvested from the world wide web and manually cropped to loosely fit to the inlying flags. More details can be found in this <a href="https://ieeexplore.ieee.org/abstract/document/6707635">paper</a>. 
      The dataset itself can be downloaded from <a href="/data/tvg/sjetley/">here</a>.
          </p>
        </div>
      </section>



      <hr class="m-0">
      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="talks">
        <div class="my-auto">
        <h2 class="mb-5">Talks</h2>
        <p class="lead mb-5">
12/18: Invited student talk on <i>robustness properties of classification CNNs</i> at <a href="https://www.wadhwaniai.org/">AI Wadhwani Institute, Mumbai</a>. Slide here.
<br/>
10/18: Invited student talk on <i>my research work and academic journey</i> at an <a href="https://www.eventbrite.com/e/women-in-ai-advancing-your-research-tickets-50703045173?utm_campaign=reminder_attendees_48hour_email&utm_medium=email&utm_source=eb_email&utm_term=eventname#">event</a> jointly organised by <a href="https://www.joinef.com/">EF</a> and <a href="https://www.researcherscode.com/">Researc/hers code</a>. Slides <a href="https://docs.google.com/presentation/d/13DurD8qbIuk28UCtX0VywebAmSjKnjOS82axhFQ9_4E/edit?usp=sharing">here</a>.
<br/>
07/17: Invited student talk on <i>real-time instance segmentation in action</i> at the Cambridge office of <a href="https://five.ai/">FiveAI</a>. Slides <a href="https://drive.google.com/file/d/1X_lZU-IkT2bdjyZ4QojfeGvodX_Tavde/view?usp=sharing">here</a>, Demo on one of the slides <a href="https://www.youtube.com/watch?v=zWZowrpaZ1c">here</a>.
<br/>
06/17: Gave a demo of our instance segmentation technology at the <a href="http://www.spacestudios.org.uk/art-technology/reconfigured-vision-workshop/">Reconfigured Vision Workshop</a> in London.
<br/>
03/17: Invited student talk on <i>real-time instance segmentation in action</i> at the <a href="http://workshops.inf.ed.ac.uk/deep/deep2017/">Deep Learning Workshop'17</a> in Edinburgh. <a href="https://www.youtube.com/watch?v=sTCY4JWPI4I">Recording</a>, <a href="https://drive.google.com/file/d/1X_lZU-IkT2bdjyZ4QojfeGvodX_Tavde/view?usp=sharing"> slides</a>, and <a href="https://www.youtube.com/watch?v=zWZowrpaZ1c">demo</a>.
<br/>

    </p>
    </section>




      <hr class="m-0">

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="teaching">
        <div class="my-auto">
          <h2 class="mb-5">Teaching</h2>

        <p class="lead mb-5">I like teaching and have tutored for the following courses at the department of Engineering Science during my PhD tenure:
        <br/><br/>
<a href="http://www.robots.ox.ac.uk/~fwood/teaching/B16_hilary_2013_2014/index.html">B16 course on Operating Systems (Hilary'16)</a>
<br/>
<a href="http://www.robots.ox.ac.uk/~az/lectures/ia/index.html">B14 course on Image Analysis (Michaelmas'16)</a>
<br/>
<a href="http://www.robots.ox.ac.uk/~sjrob/Teaching/B14_SP/b14_sp_lect1.pdf">B14 course on Signal Analysis (Hilary'17)</a>
<br/>
B14 course on Image Analysis (Exam Preparatory course, Trinity'17)
<br/>

        <p class="lead mb-5">I have also been a laboratory demonstrator for the below:
        <br/><br/>
<a href="http://www.eng.ox.ac.uk/~labejp/Courses/1P5/">P5 Matlab software programming course (Michaelmas'16, Hilary'17, Trinity'17)</a>
<br/>
B14 Lab course on Image Analysis (Hilary'17)
</p>
      </section>




      <hr class="m-0">
      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="admin">
        <div class="my-auto">
        <h2 class="mb-5">Reviewer duties</h2>
        <p class="lead mb-5">
10/18: I am reviewing for ICLR'19.
<br/>
08/18: I co-organised the <a href="http://www.deeplearningindaba.com/computer-vision-parallel-track.html">'Frontiers of Computer Vision'</a> track at the <a href="http://www.deeplearningindaba.com/2018.html"> Deep Learning Indaba'18</a>.
<br/>
07/18: I reviewed for NeurIPS'18.
</p>
    </section>



      <hr class="m-0">
      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="outreach">
        <div class="my-auto">
        <h2 class="mb-5">Outreach</h2>
        <p class="lead mb-5">
09/18: I presented my work on realtime instance segmentation at the <a href="http://www.cs.ox.ac.uk/">CS dept.</a> open-day to interested students and parents. 
<br/>
06/18: I will be the <a href="https://www.cs.ox.ac.uk/societies/women/committee.html">Outreach & Scholarship officer</a> for <a href="https://www.cs.ox.ac.uk/societies/women/">OxWoCS</a> for the academic year 2018-19.
<br/>
06/18: I co-organised a coding masterclass for girls aged 9-12 using <a href="https://www.youtube.com/watch?v=0yQYr7CIxBc&feature=youtu.be">spheros</a> as part of the <a href="https://www.cs.ox.ac.uk/conferences/InspireHer/">InspireHer!</a> series.
<br/>
05/18: I co-organised a <a href = "https://www.hayfestival.com/p-13591-bushra-alahmadi-klaudia-krawiecka-saumya-jetley.aspx">coding workshop for students aged 12-13</a> at the <a href="http://www.hayfestival.com/wales/home">Hay Science Festival</a>.
<br/>
06/17: I will be the Industry coordinator and External relations officer with <a href="https://www.cs.ox.ac.uk/societies/women/">OxWoCS</a> for the academic year 2017-18.
<br/>
05/17 to 06/17: As a student outreach ambassador for the <a href="http://www.eng.ox.ac.uk/">Department of Engineering Science, University of Oxford</a>, I have engaged with yr. 11-13 students at a host of events including - <a href="https://www.oxengscioutreach.com/events">Lubbock lectures for schools, International Women in Engineering Day and Engineering Open Days</a>. One set of slides especially popular amongst students during my classroom talks can be found <a href="./news/AI_for_visualperception_slides.html">here</a>.
<br/>
06/17: I gave a talk under the theme of 'Mobile Robotics' for the <a href="https://www.cs.ox.ac.uk/lookingforward/">Looking Forward</a> initiative of the Department of Computer Science, University of Oxford to encourage and motivate girls aged 14-15 towards STEM.
<br/>
02/17 - 05/17: I served as a student volunteer with the <a href="https://www.sciencemuseum.org.uk/what-was-on/robots">Robots exhibition at the Science Museum London</a>, supported by the Royal Academy of Engineering. More details <a href="./news/robots_SML.html">here</a>.
</p>
    </section>


      <hr class="m-0">
      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="awards">
        <div class="my-auto">
        <h2 class="mb-5">Awards</h2>
        <p class="lead mb-5">
2017: <a href="https://www.pmb.ox.ac.uk/news/pembroke-team-win-tri-innovate-2017-innovative-audio-navigation-concept">Winner of the Tri-innovate challenge</a> - Oxford university annual innovation challenge for students.
<br/>
2016: Amongst the <a href="https://warwick.ac.uk/fac/sci/dcs/research/tia/her2contest/outcome">top-10 algorithms for breast cancer prognosis challenge</a> (lookout for Team HERcules).
<br/>
2015: Best internship presentation award at Xerox Research Centre Europe, Grenoble (now <a href="http://www.europe.naverlabs.com/NAVER-LABS-Europe/People/Naila-Murray">Naver Labs</a>).
<br/>
2015: Honorary Mention for Originality for the <a href="https://drive.google.com/file/d/0B-5KrfSltQrGSXBWYkpCbThjdjQ/view">Technical Essay entry</a> at the International Computer Vision Summer School'15.
<br/>
2014-15: Recipient of the <a href="http://www.stapleytrust.org/wp/about/">Sir Richard Stapley academic scholarship</a>.
<br/>
2012: Best Performer– 2012 & Best Team – 2012 awardee at CDAC-Pune for outstanding research work.
<br/>
2010: Winner of tech innovation award at Society for Computer Science Technology and Research's <a href="http://pictinc.org/">tech symposium</a>
<br/>
2006: All India Rank 102 in 5th National Cyber Olympiad of the <a href="http://www.sofworld.org/nco">Science Olympiad Foundation</a>.
</p>
        </section>



    </div>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/resume.min.js"></script>

  </body>

</html>
